{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.sparse as sps\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer, CountVectorizer\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import svm\n",
    "from sklearn import cross_validation\n",
    "from sklearn.cross_validation import KFold\n",
    "import operator\n",
    "from collections import OrderedDict\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "from nltk.probability import FreqDist, ConditionalFreqDist\n",
    "from importlib import reload\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_path = 'train-contest.csv'\n",
    "data = pd.read_csv(train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reasons = pd.read_csv('reason.csv')\n",
    "# data['Reason'] = reasons['OpenStatus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_path = 'test-contest-second.csv'\n",
    "test_data = pd.read_csv(test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['Code'] = data.BodyMarkdown.apply(lambda text: \" \".join([line for line in text.split('\\n') if \n",
    "                                                     line.startswith('    ') or line.startswith('\\t')]))\n",
    "data['Text'] = data.BodyMarkdown.apply(lambda text: \" \".join([line for line in text.split('\\n') if not\n",
    "                                                     line.startswith('    ') and not line.startswith('\\t')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['SplittedCode'] = data.Code.apply(lambda code: re.sub(\"[^a-z]\", \" \", code.lower()).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['CodeLen'] = data.SplittedCode.apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data['Code'] = test_data.BodyMarkdown.apply(lambda text: \" \".join([line for line in text.split('\\n') if \n",
    "                                                     line.startswith('    ') or line.startswith('\\t')]))\n",
    "test_data['Text'] = test_data.BodyMarkdown.apply(lambda text: \" \".join([line for line in text.split('\\n') if not\n",
    "                                                     line.startswith('    ') and not line.startswith('\\t')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data['SplittedCode'] = test_data.Code.apply(lambda code: re.sub(\"[^a-z]\", \" \", code.lower()).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data['CodeLen'] = test_data.SplittedCode.apply(len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Текст"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['NonWords'] = data.Text.apply(lambda text: len(re.findall('[A-Za-z\\d_]+[ \\.\\?\\,\\!]', text)))\n",
    "data['Digits'] = data.Text.apply(lambda text: len(re.findall('\\d+', text)))\n",
    "data['NonAlNums'] = data.Text.apply(lambda text: len(re.findall('[^\\w\\s]+', text)))\n",
    "data['URLs'] = data.Text.apply(lambda text: len(re.findall('https?://', text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data['NonWords'] = test_data.Text.apply(lambda text: len(re.findall('[A-Za-z\\d_]+[ \\.\\?\\,\\!]', text)))\n",
    "test_data['Digits'] = test_data.Text.apply(lambda text: len(re.findall('\\d+', text)))\n",
    "test_data['NonAlNums'] = test_data.Text.apply(lambda text: len(re.findall('[^\\w\\s]+', text)))\n",
    "test_data['URLs'] = test_data.Text.apply(lambda text: len(re.findall('https?://', text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data['Text'] = data.Text.apply(nltk.word_tokenize)\n",
    "data['PureText'] = data.Text.apply(lambda text: [word for word in text if word[0].isalpha()])\n",
    "data['NumWords'] = data.PureText.apply(lambda x: len(x))\n",
    "data.Title = data.Title.apply(nltk.word_tokenize)\n",
    "data['Sentences'] = data.Text.apply(lambda x:nltk.sent_tokenize(\" \".join(x)))\n",
    "data['NumSentences'] = data.Sentences.apply(len)\n",
    "data['Questions'] = data.Sentences.apply(lambda x: len([sent for sent in x if sent[-1]=='?']))\n",
    "data['Questions_ratio'] = data.Questions / (data.NumSentences + 1)\n",
    "data.Questions_ratio = data.Questions_ratio.fillna(0)\n",
    "data['NumLastQuestions'] = data.Sentences.apply(lambda text:\n",
    "                    [sent[-1] for sent in text[-5:]].count('?'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data['Text'] = test_data.Text.apply(nltk.word_tokenize)\n",
    "test_data['PureText'] = test_data.Text.apply(lambda text: [word for word in text if word[0].isalpha()])\n",
    "test_data['NumWords'] = test_data.PureText.apply(lambda x: len(x))\n",
    "test_data.Title = test_data.Title.apply(nltk.word_tokenize)\n",
    "test_data['Sentences'] = test_data.Text.apply(lambda x:nltk.sent_tokenize(\" \".join(x)))\n",
    "test_data['NumSentences'] = test_data.Sentences.apply(len)\n",
    "test_data['Questions'] = test_data.Sentences.apply(lambda x: len([sent for sent in x if sent[-1]=='?']))\n",
    "test_data['Questions_ratio'] = test_data.Questions / (data.NumSentences + 1)\n",
    "test_data.Questions_ratio = test_data.Questions_ratio.fillna(0)\n",
    "test_data['NumLastQuestions'] = test_data.Sentences.apply(lambda text:\n",
    "                    [sent[-1] for sent in text[-5:]].count('?'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    118172.000000\n",
       "mean          0.216165\n",
       "std           0.816060\n",
       "min           0.000000\n",
       "25%           0.000000\n",
       "50%           0.142857\n",
       "75%           0.333333\n",
       "max         259.000000\n",
       "Name: Questions_ratio, dtype: float64"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.Questions_ratio.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Пользователь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/pandas/core/generic.py:2177: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self[name] = value\n"
     ]
    }
   ],
   "source": [
    "data['PostCreationDate'] = data['PostCreationDate'].apply(lambda x: time.strptime(x[:10], \"%m/%d/%Y\"))\n",
    "data['OwnerCreationDate'] = data['OwnerCreationDate'].apply(lambda x: x if (x != '2008-09-01') else '01/09/2008')\n",
    "data['OwnerCreationDate'] = data['OwnerCreationDate'].apply(lambda x: time.strptime(x[:10], \"%m/%d/%Y\"))\n",
    "data['OwnerCreationDate'] = data['OwnerCreationDate'].apply(lambda x: time.mktime(x) / 86400)\n",
    "data['PostCreationDate'] = data['PostCreationDate'].apply(lambda x: time.mktime(x) / 86400)\n",
    "data['DaysBeforePost'] = data['PostCreationDate'] - data['OwnerCreationDate']\n",
    "data[data['DaysBeforePost'] < 0].DaysBeforePost = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/pandas/core/generic.py:2177: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self[name] = value\n"
     ]
    }
   ],
   "source": [
    "test_data['PostCreationDate'] = test_data['PostCreationDate'].apply(lambda x: time.strptime(x[:10], \"%m/%d/%Y\"))\n",
    "test_data['OwnerCreationDate'] = test_data['OwnerCreationDate'].apply(lambda x: x if (x != '2008-09-01') else '01/09/2008')\n",
    "test_data['OwnerCreationDate'] = test_data['OwnerCreationDate'].apply(lambda x: time.strptime(x[:10], \"%m/%d/%Y\"))\n",
    "test_data['OwnerCreationDate'] = test_data['OwnerCreationDate'].apply(lambda x: time.mktime(x) / 86400)\n",
    "test_data['PostCreationDate'] = test_data['PostCreationDate'].apply(lambda x: time.mktime(x) / 86400)\n",
    "test_data['DaysBeforePost'] = test_data['PostCreationDate'] - data['OwnerCreationDate']\n",
    "test_data[test_data['DaysBeforePost'] < 0].DaysBeforePost = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Теги"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['Tags'] = list(zip(data.Tag1.fillna(\"\"), \n",
    "                               data.Tag2.fillna(\"\"), data.Tag3.fillna(\"\"), \n",
    "                               data.Tag4.fillna(\"\"), data.Tag5.fillna(\"\"))) \n",
    "data['Tags'] = data['Tags'].apply(lambda x: (\" \".join(x).lower().split()))\n",
    "data['EmptyTags'] = data['Tags'].apply(lambda tagset: len(tagset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data['Tags'] = list(zip(test_data.Tag1.fillna(\"\"), \n",
    "                               test_data.Tag2.fillna(\"\"), test_data.Tag3.fillna(\"\"), \n",
    "                               test_data.Tag4.fillna(\"\"), test_data.Tag5.fillna(\"\"))) \n",
    "test_data['Tags'] = test_data['Tags'].apply(lambda x: (\" \".join(x).lower().split()))\n",
    "test_data['EmptyTags'] = test_data['Tags'].apply(lambda tagset: len(tagset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Тестовая и обучающая выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_data = data[:70000]\n",
    "testing_data = data[70000:]\n",
    "train_columns = [col for col in data.columns if col != 'OpenStatus' and col != 'PostClosedDate']\n",
    "x_tr = training_data[train_columns]\n",
    "y_tr = training_data.OpenStatus\n",
    "x_test = testing_data[train_columns]\n",
    "y_test = testing_data.OpenStatus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_columns = [col for col in data.columns if col != 'OpenStatus' and col != 'PostClosedDate' and col != 'Reason']\n",
    "x_tr = data[train_columns]\n",
    "y_tr = data.OpenStatus\n",
    "x_test = test_data[train_columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 31)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_tr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(118172, 31)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##TF—IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer(lowercase=False, tokenizer=lambda doc: doc)\n",
    "text_tr_tfidf = vect.fit_transform(x_tr.Text)\n",
    "text_test_tfidf = vect.transform(x_test.Text)\n",
    "title_tr_tfidf = vect.fit_transform(x_tr.Title)\n",
    "title_test_tfidf = vect.transform(x_test.Title)\n",
    "tag_tr_tfidf = vect.fit_transform(x_tr.Tags)\n",
    "tag_test_tfidf = vect.transform(x_test.Tags)\n",
    "code_tr_tfidf = vect.fit_transform(x_tr.SplittedCode)\n",
    "code_test_tfidf = vect.transform(x_test.SplittedCode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2-граммы для названия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/pandas/core/generic.py:2177: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self[name] = value\n"
     ]
    }
   ],
   "source": [
    "x_tr.Title = x_tr.Title.apply(\" \".join)\n",
    "x_test.Title = x_test.Title.apply(\" \".join)\n",
    "bigram_vectorizer = CountVectorizer(ngram_range=(2,2), token_pattern=r'\\b\\w+\\b', min_df=1)\n",
    "bititle_tr = bigram_vectorizer.fit_transform(x_tr.Title)\n",
    "bititle_test = bigram_vectorizer.transform(x_test.Title)\n",
    "transformer = TfidfTransformer()\n",
    "bititle_test = transformer.fit_transform(bititle_test)\n",
    "bititle_tr = transformer.fit_transform(bititle_tr)\n",
    "x_tr.Title = x_tr.Title.apply(lambda text: text.split())\n",
    "x_test.Title = x_test.Title.apply(lambda text: text.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2-граммы для текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/pandas/core/generic.py:2177: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self[name] = value\n"
     ]
    }
   ],
   "source": [
    "x_tr.PureText = x_tr.PureText.apply(\" \".join)\n",
    "x_test.PureText = x_test.PureText.apply(\" \".join)\n",
    "bigram_vectorizer = CountVectorizer(ngram_range=(2,2), token_pattern=r'\\b\\w+\\b', min_df=1)\n",
    "bigrams_tr = bigram_vectorizer.fit_transform(x_tr.PureText)\n",
    "bigrams_test = bigram_vectorizer.transform(x_test.PureText)\n",
    "transformer = TfidfTransformer()\n",
    "bigrams_test = transformer.fit_transform(bigrams_test)\n",
    "bigrams_tr = transformer.fit_transform(bigrams_tr)\n",
    "x_tr.PureText = x_tr.PureText.apply(lambda text: text.split())\n",
    "x_test.PureText = x_test.PureText.apply(lambda text: text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 16129)\n",
      "(100000, 54956)\n",
      "(100000, 289729)\n",
      "(100000, 146285)\n",
      "(100000, 1409456)\n",
      "(100000, 290929)\n"
     ]
    }
   ],
   "source": [
    "print(tag_tr_tfidf.shape)\n",
    "print(title_tr_tfidf.shape)\n",
    "print(text_tr_tfidf.shape)\n",
    "print(code_tr_tfidf.shape)\n",
    "print(bigrams_tr.shape)\n",
    "print(bititle_tr.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Шкалирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def log_scaler(df):\n",
    "    new_df = df.copy()\n",
    "    quant = new_df.quantile(0.9)\n",
    "    if new_df.min() < 0:\n",
    "        new_df -= new_df.min()\n",
    "    new_df[new_df > quant] = np.log(new_df[new_df > quant] - quant + 1) + quant\n",
    "    return 0.5 + (new_df - new_df.mean()) / (new_df.std())\n",
    "\n",
    "def norm_scaler(df):\n",
    "    new_df = df.copy()\n",
    "    return 0.5 + (new_df - new_df.mean()) / new_df.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Отбор тегов, слов, биграм, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def choose_vocabulary(pos_df, neg_df, new_vocab_len):\n",
    "    word_fd = FreqDist()\n",
    "    label_word_fd = ConditionalFreqDist()\n",
    "    \n",
    "    for text in pos_df:\n",
    "        for word in text:\n",
    "            word_fd[word.lower()] += 1\n",
    "            label_word_fd['pos'][word.lower()] += 1\n",
    "    \n",
    "    for text in neg_df:\n",
    "        for word in text:\n",
    "            word_fd[word.lower()] += 1\n",
    "            label_word_fd['neg'][word.lower()] += 1\n",
    "    \n",
    "    pos_word_count = label_word_fd['pos'].N()\n",
    "    neg_word_count = label_word_fd['neg'].N()\n",
    "    total_word_count = pos_word_count + neg_word_count\n",
    "    \n",
    "    word_scores = {}\n",
    "    for word, freq in word_fd.items():\n",
    "        pos_score = BigramAssocMeasures.chi_sq(label_word_fd['pos'][word],\n",
    "            (freq, pos_word_count), total_word_count)\n",
    "        neg_score = BigramAssocMeasures.chi_sq(label_word_fd['neg'][word],\n",
    "            (freq, neg_word_count), total_word_count)\n",
    "        word_scores[word] = pos_score + neg_score\n",
    "    \n",
    "    best = sorted(word_scores.items(), key=operator.itemgetter(1))\n",
    "    bestwords = set([w for w, s in best[-new_vocab_len:]])\n",
    "    vocabulary = {}\n",
    "    for word in list(bestwords):\n",
    "        vocabulary.setdefault(word, len(vocabulary))\n",
    "    return vocabulary, best\n",
    "\n",
    "def text_to_sparse_mat(df, vocabulary, shape=None):\n",
    "    indptr, indices, dt = [0], [], []\n",
    "    for text in df:\n",
    "        for word in text:\n",
    "            if word in vocabulary.keys():\n",
    "                index = vocabulary[word]\n",
    "                indices.append(index)\n",
    "                dt.append(1)\n",
    "        indptr.append(len(indices))\n",
    "    if shape is None:\n",
    "        mat = sps.csr_matrix((dt, indices, indptr), dtype=float)\n",
    "    else:\n",
    "        mat = sps.csr_matrix((dt, indices, indptr), dtype=float, shape=shape)\n",
    "    return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def choose_bigram_vocabulary(pos_df, neg_df, new_vocab_len):\n",
    "    word_fd = FreqDist()\n",
    "    label_word_fd = ConditionalFreqDist()\n",
    "    \n",
    "    for text in pos_df:\n",
    "        bigrams_lst = list(nltk.bigrams(text))\n",
    "        for bigram in bigrams_lst:\n",
    "            word_fd[bigram] += 1\n",
    "            label_word_fd['pos'][bigram] += 1\n",
    "    \n",
    "    for text in neg_df:\n",
    "        bigrams_lst = list(nltk.bigrams(text))\n",
    "        for bigram in bigrams_lst:\n",
    "            word_fd[bigram] += 1\n",
    "            label_word_fd['neg'][bigram] += 1\n",
    "    \n",
    "    pos_word_count = label_word_fd['pos'].N()\n",
    "    neg_word_count = label_word_fd['neg'].N()\n",
    "    total_word_count = pos_word_count + neg_word_count\n",
    "    \n",
    "    word_scores = {}\n",
    "    for word, freq in word_fd.items():\n",
    "        pos_score = BigramAssocMeasures.chi_sq(label_word_fd['pos'][word],\n",
    "            (freq, pos_word_count), total_word_count)\n",
    "        neg_score = BigramAssocMeasures.chi_sq(label_word_fd['neg'][word],\n",
    "            (freq, neg_word_count), total_word_count)\n",
    "        word_scores[word] = pos_score + neg_score\n",
    "    \n",
    "    best = sorted(word_scores.items(), key=operator.itemgetter(1))\n",
    "    bestwords = set([w for w, s in best[-new_vocab_len:]])\n",
    "    vocabulary = {}\n",
    "    for word in list(bestwords):\n",
    "        vocabulary.setdefault(word, len(vocabulary))\n",
    "    return vocabulary, list(best)\n",
    "\n",
    "def text_to_sparse_mat_bigrams(df, vocabulary, shape=None):\n",
    "    indptr, indices, dt = [0], [], []\n",
    "    for text in df:\n",
    "        bigrams_lst = list(nltk.bigrams(text))\n",
    "        for bigram in bigrams_lst:\n",
    "            if bigram in vocabulary.keys():\n",
    "                index = vocabulary[bigram]\n",
    "                indices.append(index)\n",
    "                dt.append(1)\n",
    "        indptr.append(len(indices))\n",
    "    if shape is None:\n",
    "        mat = sps.csr_matrix((dt, indices, indptr), dtype=float)\n",
    "    else:\n",
    "        mat = sps.csr_matrix((dt, indices, indptr), dtype=float, shape=shape)\n",
    "    return mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Текст"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('java', 2811.93520702576),\n",
       " ('language', 3142.0871255997376),\n",
       " ('me', 3166.5549680375766),\n",
       " ('software', 3197.71838825905),\n",
       " ('learn', 3581.6487397716087),\n",
       " ('good', 3615.6163112307204),\n",
       " ('`', 4137.826665640388),\n",
       " ('programming', 4758.714800701567),\n",
       " ('you', 4788.310096527604),\n",
       " ('the', 17592.73803573673)]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary, best_words = choose_vocabulary(x_tr.Text[y_tr == 1], x_tr.Text[y_tr == 0], 1000)\n",
    "text_tr_mat = text_to_sparse_mat(x_tr.Text, vocabulary)\n",
    "text_test_mat = text_to_sparse_mat(x_test.Text, vocabulary, shape=(x_test.shape[0], len(vocabulary)))\n",
    "best_words[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transformer = TfidfTransformer()\n",
    "small_text_tr_tfidf = transformer.fit_transform(text_tr_mat)\n",
    "small_text_test_tfidf = transformer.fit_transform(text_test_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Теги"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('java', 422.1907041883227),\n",
       " ('interview-questions', 434.8354208020202),\n",
       " ('algorithm', 442.55205130741695),\n",
       " ('c', 504.77345034852806),\n",
       " ('career-development', 574.574356555618),\n",
       " ('ubuntu', 589.1049776425392),\n",
       " ('programming-languages', 621.9728027923425),\n",
       " ('linux', 969.8675520750619),\n",
       " ('books', 1147.7683700768878),\n",
       " ('php', 1580.4487033053413)]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary, best_words = choose_vocabulary(x_tr.Tags[y_tr == 1], x_tr.Tags[y_tr == 0], 100)\n",
    "tags_tr_mat = text_to_sparse_mat(x_tr.Tags, vocabulary)\n",
    "tags_test_mat = text_to_sparse_mat(x_test.Tags, vocabulary, shape=(x_test.shape[0], len(vocabulary)))\n",
    "best_words[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transformer = TfidfTransformer()\n",
    "small_tag_tr_tfidf = transformer.fit_transform(tags_tr_mat)\n",
    "small_tag_test_tfidf = transformer.fit_transform(tags_test_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Название"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('php', 789.4289349894175),\n",
       " ('software', 900.5740261519511),\n",
       " ('books', 959.1665705520272),\n",
       " (':', 1021.7472057019033),\n",
       " ('good', 1118.9832711769616),\n",
       " ('for', 1160.4768050539485),\n",
       " ('?', 1353.1409817457416),\n",
       " ('best', 1529.93390754862),\n",
       " ('programming', 1640.3303231705308),\n",
       " ('what', 2540.445275838392)]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary, best_words = choose_vocabulary(x_tr.Title[y_tr == 1], x_tr.Title[y_tr == 0], 500)\n",
    "title_tr_mat = text_to_sparse_mat(x_tr.Title, vocabulary)\n",
    "title_test_mat = text_to_sparse_mat(x_test.Title, vocabulary, shape=(x_test.shape[0], len(vocabulary)))\n",
    "best_words[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transformer = TfidfTransformer()\n",
    "small_title_tr_tfidf = transformer.fit_transform(title_tr_mat)\n",
    "small_title_test_tfidf = transformer.fit_transform(title_test_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Код"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('q', 1319.2109796448833),\n",
       " ('cout', 1359.636836483087),\n",
       " ('self', 1672.4821050780379),\n",
       " ('int', 2046.4903512333594),\n",
       " ('if', 2123.5821365560973),\n",
       " ('forced', 2597.0031314770263),\n",
       " ('id', 2644.7648953738685),\n",
       " ('n', 2908.137290794357),\n",
       " ('j', 3037.51228453655),\n",
       " ('i', 5021.5707538878605)]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary, best_words = choose_vocabulary(x_tr.SplittedCode[y_tr == 1], x_tr.SplittedCode[y_tr == 0], 200)\n",
    "code_tr_mat = text_to_sparse_mat(x_tr.SplittedCode, vocabulary)\n",
    "code_test_mat = text_to_sparse_mat(x_test.SplittedCode, vocabulary, shape=(x_test.shape[0], len(vocabulary)))\n",
    "best_words[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transformer = TfidfTransformer()\n",
    "small_code_tr_tfidf = transformer.fit_transform(code_tr_mat)\n",
    "small_code_test_tfidf = transformer.fit_transform(code_test_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##2-граммы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('help', 'me'), 1203.470177667406),\n",
       " (('to', 'the'), 1222.446196689731),\n",
       " (('like', 'this'), 1238.7950204241063),\n",
       " (('looking', 'for'), 1269.1847261387527),\n",
       " (('I', 'have'), 1383.6003763338167),\n",
       " (('I', 'get'), 1384.891810621553),\n",
       " (('way', 'to'), 1551.5879335273673),\n",
       " (('the', 'following'), 1676.5245397444996),\n",
       " (('in', 'the'), 1738.88276968588),\n",
       " (('to', 'learn'), 2344.909982748621)]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary, best_words = choose_bigram_vocabulary(x_tr.PureText[y_tr == 1], x_tr.PureText[y_tr == 0], 200000)\n",
    "bigrams_tr_mat = text_to_sparse_mat_bigrams(x_tr.PureText, vocabulary, shape=(len(y_tr), len(vocabulary)))\n",
    "bigrams_test_mat = text_to_sparse_mat_bigrams(x_test.PureText, vocabulary, shape=(x_test.shape[0], len(vocabulary)))\n",
    "best_words[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transformer = TfidfTransformer()\n",
    "big_bigrams_tr_tfidf = transformer.fit_transform(bigrams_tr_mat)\n",
    "big_bigrams_test_tfidf = transformer.fit_transform(bigrams_test_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('help', 'me'), 1203.470177667406),\n",
       " (('to', 'the'), 1222.446196689731),\n",
       " (('like', 'this'), 1238.7950204241063),\n",
       " (('looking', 'for'), 1269.1847261387527),\n",
       " (('I', 'have'), 1383.6003763338167),\n",
       " (('I', 'get'), 1384.891810621553),\n",
       " (('way', 'to'), 1551.5879335273673),\n",
       " (('the', 'following'), 1676.5245397444996),\n",
       " (('in', 'the'), 1738.88276968588),\n",
       " (('to', 'learn'), 2344.909982748621)]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary, best_words = choose_bigram_vocabulary(x_tr.PureText[y_tr == 1], x_tr.PureText[y_tr == 0], 500)\n",
    "bigrams_tr_mat = text_to_sparse_mat_bigrams(x_tr.PureText, vocabulary, shape=(len(y_tr), len(vocabulary)))\n",
    "bigrams_test_mat = text_to_sparse_mat_bigrams(x_test.PureText, vocabulary, shape=(x_test.shape[0], len(vocabulary)))\n",
    "best_words[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transformer = TfidfTransformer()\n",
    "small_bigrams_tr_tfidf = transformer.fit_transform(bigrams_tr_mat)\n",
    "small_bigrams_test_tfidf = transformer.fit_transform(bigrams_test_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.isnan(x_test['Questions_ratio']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Логистическая Регрессия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "cols = ['CodeLen', 'NumWords', 'NumSentences', 'Questions_ratio',  'NonWords', 'Digits',\n",
    "             'NonAlNums', 'EmptyTags', 'NumLastQuestions', 'ReputationAtPostCreation', #'DaysBeforePost',\n",
    "             'OwnerUndeletedAnswerCountAtPostTime']\n",
    "cols_norm = cols\n",
    "cols_log = cols\n",
    "\n",
    "x_train_2 = np.array(norm_scaler(x_tr[cols_norm[0]]))[:, None]\n",
    "x_test_2 = np.array(norm_scaler(x_test[cols_norm[0]]))[:, None]\n",
    "for col in cols_norm[1:]:\n",
    "    x_train_2 = np.hstack((x_train_2, np.array(norm_scaler(x_tr[col]))[:, None]))\n",
    "    x_test_2 = np.hstack((x_test_2, np.array(norm_scaler(x_test[col]))[:, None]))\n",
    "for col in cols_log:\n",
    "    x_train_2 = np.hstack((x_train_2, np.array(log_scaler(x_tr[col]))[:, None]))\n",
    "    x_test_2 = np.hstack((x_test_2, np.array(log_scaler(x_test[col]))[:, None]))\n",
    "print(np.sum(np.isnan(x_test_2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_train_2 = sps.csr_matrix(sps.hstack((x_train_2, tag_tr_tfidf, title_tr_tfidf, text_tr_tfidf, code_tr_tfidf, \n",
    "                                       big_bigrams_tr_tfidf, bititle_tr)))\n",
    "x_test_2 = sps.csr_matrix(sps.hstack((x_test_2, tag_test_tfidf, title_test_tfidf, text_test_tfidf, code_test_tfidf, \n",
    "                                      big_bigrams_test_tfidf, bititle_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(118172, 998050)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf_linear = LogisticRegression()\n",
    "clf_linear.fit(x_train_2, y_tr)\n",
    "predicted_y_test = clf_linear.predict_proba(x_test_2)\n",
    "# print(roc_auc_score(y_test, predicted_y_test[:, 1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(118172, 2)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv('sample-submission-second.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PostId</th>\n",
       "      <th>OpenStatus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PostId  OpenStatus\n",
       "0       1           0\n",
       "1       2           0\n",
       "2       3           0\n",
       "3       4           0\n",
       "4       5           0"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "answers = test_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "answers['OpenStatus'] = predicted_y_test[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "answers.to_csv(\"answers.csv\", columns=['PostId', 'OpenStatus'], index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PostId</th>\n",
       "      <th>OpenStatus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.898929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.848850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.765294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.697102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.626402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PostId  OpenStatus\n",
       "0       1    0.898929\n",
       "1       2    0.848850\n",
       "2       3    0.765294\n",
       "3       4    0.697102\n",
       "4       5    0.626402"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('answers.csv').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Попробуем логистическую регрессию только по тексту"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "cols = ['CodeLen', 'NumWords', 'NumSentences', 'Questions_ratio',  'NonWords', 'Digits', \n",
    "             'NonAlNums', 'EmptyTags', 'NumLastQuestions']\n",
    "cols_norm = cols\n",
    "cols_log = cols\n",
    "\n",
    "x_train_small = np.array(norm_scaler(x_tr[cols_norm[0]]))[:, None]\n",
    "x_test_small = np.array(norm_scaler(x_test[cols_norm[0]]))[:, None]\n",
    "for col in cols_norm[1:]:\n",
    "    x_train_small = np.hstack((x_train_small, np.array(norm_scaler(x_tr[col]))[:, None]))\n",
    "    x_test_small = np.hstack((x_test_small, np.array(norm_scaler(x_test[col]))[:, None]))\n",
    "for col in cols_log:\n",
    "    x_train_small = np.hstack((x_train_small, np.array(log_scaler(x_tr[col]))[:, None]))\n",
    "    x_test_small = np.hstack((x_test_small, np.array(log_scaler(x_test[col]))[:, None]))\n",
    "print(np.sum(np.isnan(x_test_small)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train_small = sps.csr_matrix(sps.hstack((x_train_small, small_tag_tr_tfidf, small_title_tr_tfidf, small_text_tr_tfidf, \n",
    "                                       small_code_tr_tfidf, \n",
    "                                       small_bigrams_tr_tfidf)))\n",
    "x_test_small = sps.csr_matrix(sps.hstack((x_test_small, small_tag_test_tfidf, small_title_test_tfidf, small_text_test_tfidf, \n",
    "                                      small_code_test_tfidf, \n",
    "                                      small_bigrams_test_tfidf)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 2318)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_small.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.834042481021\n"
     ]
    }
   ],
   "source": [
    "clf_linear = LogisticRegression()\n",
    "clf_linear.fit(x_train_small, y_tr)\n",
    "text_y_test = clf_linear.predict_proba(x_test_small)\n",
    "print(roc_auc_score(y_test, text_y_test[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.809514552368\n",
      "3.766116142272949\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=700, n_jobs=-1)\n",
    "rf.fit(x_train_small, y_tr)\n",
    "start = time.time()\n",
    "rf_y_test = rf.predict_proba(x_test_small)\n",
    "print(roc_auc_score(y_test, rf_y_test[:, 1]))\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.851532999541\n",
      "0.852873288138\n",
      "0.853531964642\n",
      "0.853306899873\n",
      "0.851878764483\n",
      "0.8488042845\n",
      "0.843587614766\n",
      "0.835575747674\n",
      "0.824305613667\n",
      "0.809514552368\n"
     ]
    }
   ],
   "source": [
    "for w in np.linspace(0, 1, 10):\n",
    "    print(roc_auc_score(y_test, w * rf_y_test[:, 1] + (1 - w) * predicted_y_test[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Исследование ошибок"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_results(prediction):\n",
    "    reasons_lst = ['open', 'not a real question', 'off topic', 'not constructive', 'too localized']\n",
    "    sizes = [len(x_test[(y_test != np.round(prediction[:, 1])) & \n",
    "                        (x_test.Reason == reason)])/len(y_test[x_test.Reason == reason])\n",
    "             for reason in reasons_lst]\n",
    "    for i in range(5):\n",
    "        print(reasons_lst[i],':', sizes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "open : 0.23026575191230975\n",
      "not a real question : 0.2282088122605364\n",
      "off topic : 0.21756487025948104\n",
      "not constructive : 0.23104227170054856\n",
      "too localized : 0.22473118279569892\n"
     ]
    }
   ],
   "source": [
    "plot_results(predicted_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.62586858  0.37413142]\n",
      "10\n",
      "1384 324.0\n",
      "I 'm tearing my hair out . Apparently you ca n't just do something like I 've searched the internet and I ca n't find a simple example of how to implement the damn thing ANYWHERE . Even MSDN . It 's ridiculous .\n",
      "    class Ranch<T> : IEnumerable<T>\r",
      "     {\r",
      "        // Mad code\r",
      "         IEnumerator<T> GetEnumerator()\r",
      "         {\r",
      "             // Hectic implementation details\r",
      "         }\r",
      "        // More mad code\r",
      "     }\r\n",
      "[ 0.62586858  0.37413142]\n",
      "11\n",
      "3 2.0\n",
      "I searched a lot but could n't find the way to measure web page loading time with iOS . In the app , I want to show certain page loading time.. Is it possible with iOS sdk or third party sdk ? Thanks\n",
      "\n",
      "[ 0.62586858  0.37413142]\n",
      "12\n",
      "97 318.0\n",
      "I have downloaded 'Adobe Flash builder for Force.com ' from Salesforce site and trying to get some hands on . If any one has tried it before request you to please share your experience . Is there seom kind of good tutorial available that shows step by step procedure to create flex SFDC projects in eclipse ( Flas Builder 4 plugin versoin ) from scratch .\n",
      "\n",
      "[ 0.62586858  0.37413142]\n",
      "13\n",
      "1 48.9583333333\n",
      "Hey , So i have about 4-5 years of background in programming some in C # and some in C++ . I recently got an internship and have been using C # daily . I am confident in my work , and do n't have any problem remembering syntax or anything like that . So i was wondering what you guys think about getting reshaper ? i was going to demo it first but i just want to hear other opinions about it . Basically what I 'm trying to ask is , should i wait to get it and become more experienced and have more practice with just visual studios and its built in intellisense and stuff or would it be alright to get it ?\n",
      "\n",
      "[ 0.62586858  0.37413142]\n",
      "14\n",
      "744 396.0\n",
      "I 've been scouring the internet for a guide to creating an xbox application ( not a game ) but have been unsuccessful so far . I 've found that Microsoft provides XNA and XDK , and that the XDK is only available through their developer program . Is doesnt seem like it is possible to create an application by just using the XNA , however , I am not sure if this is correct or not . It does look like the XNA will allow you to create a game , but I 'm not sure if this is the right development kit to use if youre only creating an application . So , can I use the XNA to create an xbox application or do I need to do something else for this ? Thanks !\n",
      "\n",
      "[ 0.62586858  0.37413142]\n",
      "15\n",
      "1 0.0\n",
      "i am getting error like No fault address is defined for fault message can u help me < pre > & lt ; ds : direct bindOnElement= '' # document '' datasource= '' dbstore '' driver= '' com.mysql.jdbc.Driver '' url= '' jdbc : mysql : //localhost/mydb '' username= '' root '' password= '' root '' autoCommit= '' false '' / & gt ; & lt ; db : executor executeOnElement= '' /csv-set/csv-record '' datasource= '' dbstore '' executeBefore= '' true '' & gt ; & lt ; db : statement & gt ; INSERT INTO ctable VALUES ( $ { Customer.firstName ? string } , $ { Customer.lastName ? string } , $ { Customer.city ? string } , $ { Customer.country ? string } ) & lt ; /db : statement & gt ; & lt ; /db : executor & gt ; < /pre >\n",
      "\n",
      "[ 0.62586858  0.37413142]\n",
      "16\n",
      "854 244.0\n",
      "Very much new to this issue , need to fine total travelling distance while going from one location to another and also need to count total time spent to that travel.finding distance using latitude and longitude of two location do n't give correct result i think , because lat and lang distance gives us straight line distance even if we have lots of turning points etc ... while we are travelling . http : //stackoverflow.com/questions/3109158/how-to-draw-a-path-on-a-map-using-kml-file/3109723 # 3109723 i see this issue , good example has been given there i have implemented that also but it 's complicated bcs i do n't need to draw a routing path i just need to count total distance and total time . So Hoping for some one will helpme Thanks in advance Aamirkhan I .\n",
      "\n",
      "[ 0.62586858  0.37413142]\n",
      "17\n",
      "1 281.0\n",
      "Can anyone tell me on how to create 2 different project thro coding as Projectname_MIDP and Projectname_RIM like its created in lwuit1.5 GUI Builder automatically . Thanks\n",
      "\n",
      "[ 0.62586858  0.37413142]\n",
      "18\n",
      "462 6.04166666667\n",
      "We have always had languages that were preferable to be used in a particular scenario . For a quick prototype development , VB6 was an obvious choice . VB6 was chosen in projects that had a simple desktop user interface and standard and un-complicated database interaction requirements . If you wanted to develop a device driver using low-level routines , you probably relied on C or Visual C++ . ASP was a standard choice for development of web interfaces . Every language had a particular 'domain ' or 'specialization ' , speaking crudely . With .NET framework , all languages are interoperable and presumably consistent . You can have a project with modules from different languages all together but all ultimately being treated fairly similary ( all get compiled to IL ) . Does this mean that the distinction we had earlier no longer exists ? That differentiation was not necessarily bad rather something that was there by design and not due to any constraint . That apparently is diminshed somewhat with the .NET framework and its treatment of various languages . What is the experience of everyone , specially if you have worked in both managed and non-managed environments ?\n",
      "\n",
      "[ 0.62586858  0.37413142]\n",
      "19\n",
      "1381 27.0\n",
      "I have a system that combines the best and worst of Java and PHP . I am trying to migrate a component that was once written in PHP into a Java One . Does anyone have some tips for how I can parse a PHP serialized datastructure in Java ? By serialized I mean output from php 's serialize function .\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/IPython/kernel/__main__.py:4: FutureWarning: in the future, boolean array-likes will be handled as a boolean array index\n"
     ]
    }
   ],
   "source": [
    "noquest = x_test[(y_test != np.round(predicted_y_test[:, 1])) & (x_test.Reason == 'not constructive')] \n",
    "noquest.index = range(len(noquest))\n",
    "for i in range(10, 20): \n",
    "    print((predicted_y_test[(y_test != np.round(predicted_y_test[:, 1])) & (x_test.Reason == 'not constructive')])[i])\n",
    "    print(i)\n",
    "    print(noquest.ReputationAtPostCreation[i], noquest.DaysBeforePost[i])\n",
    "    print(' '.join(noquest.Text[i]))\n",
    "    print(noquest.Code[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "547"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_test[(y_test != np.round(predicted_y_test[:, 1])) & (y_test != np.round(text_y_test[:, 1]))\n",
    "           & (x_test.Reason == 'not constructive')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "733"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_test[(y_test != np.round(text_y_test[:, 1])) & (x_test.Reason == 'not constructive')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
